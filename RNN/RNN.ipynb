{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","execution_count":null,"source":["# Importing Libraries\n","\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt \n","\n","import time\n","import os\n","import re\n","\n","from tqdm.notebook import tqdm"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"code","execution_count":null,"source":["##### Configuration\n","\n","class CFG:\n","    PATH = '../input/harry-potter/'\n","    MAX_LEN = 100\n","    GRAD_MIN = -5\n","    GRAD_MAX = 5\n","    HIDDEN_SIZE = 256\n","    LR = 3e-4\n","    TRAIN_SIZE_FRACTION = 0.9\n","    EPOCHS = 5\n","    TEMPERATURE = 0.5\n","    \n","    SEED = 42\n","    \n","    BETA1 = 0.9\n","    BETA2 = 0.999\n","    EPS = 1e-8\n","    USE_ADAM = True"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# For Reproducing results\n","\n","def seed_everything(seed):\n","    np.random.seed(seed)\n","\n","seed_everything(CFG.SEED)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Function to load book at 'path'\n","\n","def load_book(path):\n","    with open(path, encoding='latin1') as f:\n","        book = f.read()\n","    return book"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Loading books\n","\n","book_names = os.listdir(CFG.PATH)\n","books = [load_book(CFG.PATH+book_name) for book_name in book_names]\n","print(\"Number of books: \",len(books))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Preliminary Analysis for preprocessing\n","\n","for i in range(len(books)):\n","    if i==0:\n","        s = set(books[i])\n","    else:\n","        s.update(set(books[i]))\n","\n","print('Set of unique characters: ', end='\\n\\n')\n","print(s)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Preprocess text\n","\n","def preprocess(text):\n","\n","    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n","    text = re.sub('a0','', text)\n","    text = re.sub('\\'92t','\\'t', text)\n","    text = re.sub('\\'92s','\\'s', text)\n","    text = re.sub('\\'92m','\\'m', text)\n","    text = re.sub('\\'92ll','\\'ll', text)\n","    text = re.sub('\\'91','', text)\n","    text = re.sub('\\'92','', text)\n","    text = re.sub('\\x93','',text)  \n","    text = re.sub('\\x96','',text)  \n","    text = re.sub('\\'93','', text)\n","    text = re.sub('\\'94','', text)\n","    text = re.sub('\\.','. ', text)\n","    text = re.sub('\\!','! ', text)\n","    text = re.sub('\\?','? ', text)\n","    text = re.sub(' +',' ', text)\n","    text = re.sub(r'[`^~\\x90\\x91\\x92\\x95\\xad\\x1f¦$«»éü0123456789/]', '', text)\n","    \n","    return text"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Preprocessing books\n","\n","cleaned_books = []\n","\n","for book in books:\n","    cleaned_books.append(preprocess(book))\n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Doing a quick check on preprocessing\n","\n","print(books[0][:1000])\n","print()\n","print('###########')\n","print()\n","print(cleaned_books[0][:1000])"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Building Vocabulary\n","\n","char_to_int = {}\n","idx = 0\n","\n","for book in cleaned_books:\n","    for char in book:\n","        if char not in char_to_int:\n","            char_to_int[char] = idx\n","            idx += 1\n","\n","int_to_char = {}\n","for char, idx in char_to_int.items():\n","    int_to_char[idx] = char"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Some variables to use later\n","\n","vocab_size = len(int_to_char)\n","CFG.VOCAB_SIZE = vocab_size\n","\n","all_unique_chars = list(char_to_int.keys())\n","all_unique_chars.sort()\n","\n","print('Set of unique characters after preprocessing:', end='\\n\\n')\n","print(all_unique_chars)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Defining inputs and targets for our language model\n","\n","inputs = []\n","targets = []\n","\n","for book in cleaned_books:\n","    i = 0\n","    \n","    while(i + CFG.MAX_LEN < len(book)):\n","        \n","        inputs.append(book[i : i + CFG.MAX_LEN])\n","        targets.append(book[(i+1) : (i+1) + CFG.MAX_LEN])\n","        i += CFG.MAX_LEN\n","\n","print('Total Number of Data Points:')\n","print(len(inputs), '\\n')\n","print(\"First 5 input/target pairs\", '\\n')\n","print(inputs[:5])\n","print()\n","print(targets[:5])"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Tokenizing inputs and targets to feed to model\n","\n","int_inputs = []\n","int_targets = []\n","\n","print('Tokenization and Integer representation...')\n","for i in tqdm(range(len(inputs))):\n","    int_input = []\n","    int_target = []\n","    \n","    input = inputs[i]\n","    target = targets[i]\n","    \n","    for char_i in range(len(input)):\n","        int_input.append(char_to_int[input[char_i]])\n","        int_target.append(char_to_int[target[char_i]])        \n","    \n","    int_inputs.append(int_input)\n","    int_targets.append(int_target)    \n"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Sanity check for inputs/targets\n","\n","print(int_inputs[0][:10])\n","print(int_targets[0][:10])"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Train-Validation split\n","\n","cnt = CFG.TRAIN_SIZE_FRACTION * len(int_inputs)\n","\n","train_inputs = int_inputs[ : int(cnt)]\n","train_targets = int_targets[ : int(cnt)]\n","\n","valid_inputs = int_inputs[int(cnt) : ]\n","valid_targets = int_targets[int(cnt) : ]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Batch generator\n","\n","def get_batch(inputs, targets):\n","    \n","    for batch_num in range(len(inputs)):\n","        input = inputs[batch_num]\n","        target = targets[batch_num]        \n","        \n","        yield(input, target)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["class RNN():\n","    \n","    def __init__(self, vocab_size, hidden_size, max_len):\n","        '''\n","            DESCRIPTION: \n","                Initializing Weights for RNN cell\n","            INPUTS:\n","                vocab_size: Size of vocabulary\n","                hidden_size: Size of hidden nodes( RNN cells )\n","                max_len: Length of input sequence \n","        '''\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.max_len = max_len\n","        \n","        # Initializing parameters by taking reference from Bengio's paper : \"Understanding the difficulty of training deep feedforward neural networks [Xavier, Bengio]\"\n","        self.U = np.random.uniform(- np.sqrt(1. / vocab_size), np.sqrt(1. / vocab_size),\n","                                  size = (hidden_size, vocab_size))\n","        \n","        self.W = np.random.uniform(- np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                  size = (hidden_size, hidden_size))\n","        \n","        self.V = np.random.uniform(- np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n","                                  size = (vocab_size, hidden_size))\n","        \n","        self.b = np.zeros(shape = (hidden_size, 1))\n","        self.c = np.zeros(shape = (vocab_size, 1))\n","        \n","        # Parameters for Adam\n","        self.mU, self.vU = np.zeros_like(self.U), np.zeros_like(self.U)\n","        self.mW, self.vW = np.zeros_like(self.W), np.zeros_like(self.W)\n","        self.mV, self.vV = np.zeros_like(self.V), np.zeros_like(self.V)\n","        self.mb, self.vb = np.zeros_like(self.b), np.zeros_like(self.b)\n","        self.mc, self.vc = np.zeros_like(self.c), np.zeros_like(self.c)\n","        \n","    \n","    def forward(self, input, h_prev):\n","        '''\n","            Fordward prop. for a fixed number of time-steps\n","        '''\n","        xs, hs, os, ypreds = {}, {}, {}, {}\n","        hs[-1] = np.copy(h_prev)\n","        \n","        for t in range(len(input)):\n","            \n","            xs[t] = np.zeros((self.vocab_size, 1))\n","            xs[t][input[t]] = 1                                                          # one-hot representation of input char\n","            hs[t] = np.tanh(np.dot(self.W, hs[t-1]) + np.dot(self.U, xs[t]) + self.b)    # hidden state: h(t) = tanh(W*h(t-1) + U*X(t))\n","            os[t] = np.dot(self.V, hs[t]) + self.c                                       # Output: o(t) = V*h(t) + c\n","            ypreds[t] = self.softmax(os[t])                                              # Predicted: softmax(o(t)) for non-max supression\n","            \n","        return xs, hs, os, ypreds\n","        \n","    \n","    def softmax(self, x):\n","        '''\n","            Computes softmax of vector x of any dimension\n","        '''\n","        p = np.exp(x - np.max(x))\n","        return p / np.sum(p)\n","    \n","    \n","    def backward(self, xs, hs, ps, ys):\n","        '''\n","        Arguments are computed using forward pass\n","            \n","            xs : input embeddings for each time step\n","            hs : hidden states for each time step\n","            ps : normalized probabilities (softmax) for each time step\n","            ys : True targets for each time step\n","        '''\n","        dU = np.zeros_like(self.U)\n","        dW = np.zeros_like(self.W)\n","        dV = np.zeros_like(self.V)\n","        db = np.zeros_like(self.b)\n","        dc = np.zeros_like(self.c)\n","        \n","        # For last time step, dhnext is zero\n","        dhnext = np.zeros_like(hs[0])\n","        \n","        # Backpropagating in time from the last time step\n","        for t in reversed(range(self.max_len)):\n","            \n","            # Derivative of Loss wrt output o\n","            dy = np.copy(ps[t])\n","            dy[ys[t]] -= 1\n","            \n","            dc += dy\n","            dV += np.dot(dy, hs[t].T)\n","            dh = np.dot(self.V.T, dy) + dhnext\n","            \n","            dh_inter = (1 - hs[t] * hs[t]) * dh\n","            \n","            db += dh_inter\n","            dW += np.dot(dh_inter, hs[t-1].T)\n","            dU += np.dot(dh_inter, xs[t].T) \n","            \n","            # Updating del L/ del h(t+1) for intermediate layers\n","            dhnext = np.dot(self.W.T, dh_inter)\n","        \n","        # Clipping off large gradient values\n","        for param in [dW, dV, dU, db, dc]:\n","            np.clip(param, CFG.GRAD_MIN, CFG.GRAD_MAX, out = param)\n","        \n","        return dW, dV, dU, db, dc\n","    \n","    \n","    def update_weights(self, t, dW, dV, dU, db, dc):\n","        '''\n","            Updating weights using:    SGD / ADAM\n","        '''\n","        if CFG.USE_ADAM:\n","            # ADAM optimizer used\n","            for param, grad, m, v in zip( [self.W, self.V, self.U, self.b, self.c],\n","                                          [dW,      dV,         dU,      db,   dc],\n","                                          [self.mW, self.mV, self.mU, self.mb, self.mc],\n","                                          [self.vW, self.vV, self.vU, self.vb, self.vc]):\n","\n","\n","                m = CFG.BETA1 * m + (1. - CFG.BETA1) * grad\n","                mt = m / (1. - CFG.BETA1**t)\n","                v = CFG.BETA2 * v + (1 - CFG.BETA2) * (grad**2)\n","                vt = v / (1. - CFG.BETA2**t)\n","\n","                # Adam update\n","                param += - CFG.LR * mt / (np.sqrt(vt) + CFG.EPS)\n","        \n","        else:\n","            # SGD optimizer\n","            for param, grad in zip( [self.W, self.V, self.U, self.b, self.c],\n","                                   [dW,      dV,         dU,      db,   dc]):\n","                \n","                param += - CFG.LR * grad"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Softmax loss/ Multiclass cross-entropy loss\n","\n","def criterion(ps, ys):\n","    '''\n","        ps: normalized probabilities (softmax) for each time step\n","        ys: True targets for each time step\n","    '''\n","    return sum(-np.log(ps[t][ys[t], 0]) for t in range(CFG.MAX_LEN))\n","    "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Initializing model object\n","\n","model = RNN(vocab_size = CFG.VOCAB_SIZE, hidden_size = CFG.HIDDEN_SIZE,\n","            max_len = CFG.MAX_LEN)\n","\n","\n","\n","def accuracy(ypreds, target):\n","    '''\n","        Computes accuracy of output sequence\n","    '''\n","    preds = []\n","    for t in range(len(ypreds)):\n","        '''Sampling a word from computed probability distribution over entire vocabulary'''\n","        preds.append(np.random.choice(range(CFG.VOCAB_SIZE), p = ypreds[t].ravel()))\n","        \n","    acc = 0\n","    for i in range(len(preds)):\n","        acc += preds[i]==target[i]\n","    \n","    return acc*100. / len(preds)\n","\n","\n","\n","def get_sample(model, h, start_idx, n):\n","    '''\n","        Generated sample text given an input character\n","    '''\n","    x = np.zeros(shape = (CFG.VOCAB_SIZE, 1))\n","    x[start_idx] = 1\n","    result = []\n","    \n","    for t in range(n):\n","        \n","        h = np.tanh(np.dot(model.W, h) + np.dot(model.U, x) + model.b)    \n","        o = np.dot(model.V, h) + model.c  \n","        \n","        # Sampling using temperature to change model confidence\n","        o = o / CFG.TEMPERATURE\n","        \n","        # Subtracting max of unnormalized probabilities, to prevent NaNs in softmax\n","        o = o - np.max(o)\n","        p = np.exp(o) / np.sum(np.exp(o))\n","        \n","        # Sampling a word from computed probability distribution over entire vocabulary\n","        idx = np.random.choice(range(CFG.VOCAB_SIZE), p = p.ravel())\n","\n","        x = np.zeros(shape = (CFG.VOCAB_SIZE, 1))\n","        x[idx] = 1\n","        \n","        result.append(idx)\n","    \n","    txt = ''.join(int_to_char[i] for i in result)\n","    \n","    return txt\n","\n","\n","\n","def train():\n","    \n","    train_loss = []\n","    train_acc = []\n","    eval_loss = []\n","    \n","    iter_ = 1\n","    start = time.time()\n","    \n","    for epoch in range(CFG.EPOCHS):\n","        \n","        epoch_loss = 0\n","        epoch_acc = 0\n","        \n","        #training on small subset\n","        batch = enumerate(get_batch(train_inputs, train_targets))\n","        \n","        # Initialize prev hidden state with 0 before every epoch\n","        hprev = np.zeros(shape = (CFG.HIDDEN_SIZE, 1))\n","        \n","        for i, (input, target) in tqdm(batch):\n","            \n","            xs, hs, os, ypreds = model.forward(input, hprev)\n","            loss = criterion(ypreds, target)\n","            acc = accuracy(ypreds, target)\n","            epoch_loss += loss / len(train_inputs)\n","            epoch_acc += acc / len(train_inputs)\n","            \n","            dW, dV, dU, db, dc = model.backward(xs, hs, ypreds, target)\n","            model.update_weights(iter_, dW, dV, dU, db, dc)\n","\n","            # Storing hprev as prev epoch's last hidden state\n","            hprev = hs[CFG.MAX_LEN-1]\n","            iter_ += 1\n","            \n","            if iter_%500 == 0:\n","                sample = get_sample(model, hprev, input[0], CFG.MAX_LEN)\n","                \n","                print()\n","                print('Input:')\n","                inp = ''.join([int_to_char[idx] for idx in input])\n","                print(inp)\n","                print()\n","                print(\"SAMPLE:\")\n","                print(sample)\n","                print()\n","                print(\"TARGET\")\n","                true = ''.join([int_to_char[idx] for idx in target])\n","                print(true)\n","                print()\n","#                 acc = _accuracy(sample, true)\n","                print(f\"ITER: {iter_+1}\\t\\tLoss: {loss}\")\n","                print()\n","                print()\n","                \n","        \n","        print()\n","        print('###############')\n","        print(f\"EPOCH: {epoch+1}\\tTRAIN_LOSS: {epoch_loss}\\tTRAIN_ACC: {epoch_acc}\")\n","        print('#############')\n","        print()\n","        train_loss.append(epoch_loss)\n","        train_acc.append(epoch_acc)\n","    \n","    print(f'TOTAL TIME TAKEN for {CFG.EPOCHS} epochs: {time.time()-start}')\n","    return train_loss, train_acc, hprev\n","        \n","        "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["train_loss, train_acc, hs, grads = train()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["plt.plot(train_loss)\n","plt.title('Training loss')\n","plt.show()\n","plt.plot(train_acc)\n","plt.title('Training acc')\n","plt.show()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["def generate(model, hprev, initial_text, n = 1000):\n","    '''\n","        Generates a sentence of length 'n' given initial words\n","    '''\n","\n","    input = [char_to_int[char] for char in initial_text]\n","    \n","    xs, hs, os, ypreds = model.forward(input, hprev)\n","    \n","    result = input\n","\n","    for t in range(len(input) - 1, len(input) - 1 + n):\n","        \n","        hs[t] = np.tanh(np.dot(model.W, hs[t-1]) + np.dot(model.U, xs[t]) + model.b)    \n","        os[t] = np.dot(model.V, hs[t]) + model.c  \n","        \n","        ypreds[t] = model.softmax(os[t])\n","        idx = np.random.choice(range(CFG.VOCAB_SIZE), p=ypreds[t].ravel())\n","        xs[t+1] = np.zeros(shape = (CFG.VOCAB_SIZE, 1))\n","        xs[t+1][idx] = 1\n","        result.append(idx)\n","\n","    txt = ''.join(int_to_char[idx] for idx in result)\n","    \n","    return txt"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["for short_text in ['\"alohomora\" opened', 'After all this time ?', 'Wingardium Leviosa', 'Prakashit Bhavah', 'Voldemort killed']:\n","    \n","    hprev = np.zeros(shape = (CFG.HIDDEN_SIZE, 1))\n","    result = generate(model, hprev, short_text)\n","    print('INPUT STRING: \\t', short_text)\n","    print()\n","    print('GENERATED: \\n')\n","    print(result)\n","    print()\n","    print('#####'*30)\n","    print()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}]}